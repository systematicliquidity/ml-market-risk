{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "from datetime import datetime as dt\n",
    "from math import log, sqrt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy import linalg\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.utils.extmath import fast_logdet\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set Normality Test\n",
      "Valid Set Normality Test\n",
      "0001.HK,(6.064224749785349, 0.048213685104578796)\n",
      "0857.HK,(8.452645800870712, 0.014605999639945466)\n",
      "Test  Set Normality Test\n",
      "0011.HK,(3.2554994473743633, 0.19637096624740646)\n",
      "0083.HK,(6.814479085838839, 0.03313253519634285)\n",
      "0386.HK,(1.5497081805806185, 0.4607710070582528)\n",
      "1928.HK,(4.337978009576364, 0.11429310830561656)\n",
      "3328.HK,(1.1835132218824163, 0.5533544020994401)\n",
      "3988.HK,(0.18692085297707353, 0.9107740592407177)\n",
      "Train Set Normality Test\n",
      "Valid Set Normality Test\n",
      "0001.HK,(6.064224749785349, 0.048213685104578796)\n",
      "0857.HK,(8.452645800870712, 0.014605999639945466)\n",
      "Test  Set Normality Test\n",
      "0011.HK,(3.2554994473743633, 0.19637096624740646)\n",
      "0083.HK,(6.814479085838839, 0.03313253519634285)\n",
      "0386.HK,(1.5497081805806185, 0.4607710070582528)\n",
      "1928.HK,(4.337978009576364, 0.11429310830561656)\n",
      "3328.HK,(1.1835132218824163, 0.5533544020994401)\n",
      "3988.HK,(0.18692085297707353, 0.9107740592407177)\n"
     ]
    }
   ],
   "source": [
    "%run TrainValidTestDataPrep.ipynb\n",
    "print(\"Train Set Normality Test\")\n",
    "for c in data_train.columns:\n",
    "    _stats = stats.jarque_bera(data_train)\n",
    "    if np.abs(_stats[1] > 0.01): print(\"%s,%s\"%(c, _stats))\n",
    "print(\"Valid Set Normality Test\")\n",
    "for c in data_valid.columns:\n",
    "    _stats = stats.jarque_bera(data_valid[c])\n",
    "    if np.abs(_stats[1] > 0.01): print(\"%s,%s\"%(c, _stats))\n",
    "print(\"Test  Set Normality Test\")\n",
    "for c in data_test.columns:\n",
    "    _stats = stats.jarque_bera(data_test[c])\n",
    "    if np.abs(_stats[1] > 0.01): print(\"%s,%s\"%(c, _stats))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_factor_cov_from_transform(model,data_x):\n",
    "    return np.cov(model.transform(data_x).T)\n",
    "\n",
    "\n",
    "def model_to_cov_common(model,factor_cov):\n",
    "    '''pca model -> n x n covariance matrix from common factors '''\n",
    "    return np.dot(np.dot(model.components_.T,factor_cov),model.components_)\n",
    "\n",
    "\n",
    "def cov_common_to_corr(cov_common,stdev_marginals):\n",
    "    corr = cov_common / np.outer(stdev_marginals,stdev_marginals)\n",
    "    np.fill_diagonal(corr,1.0)\n",
    "    return corr\n",
    "\n",
    "\n",
    "def sample_to_stdev_marginals(data_x):\n",
    "    return np.std(data_x,axis=0)\n",
    "\n",
    "    \n",
    "def corr_to_total_cov(corr,stdev_marginals):\n",
    "    return corr * np.outer(stdev_marginals,stdev_marginals)\n",
    "\n",
    "\n",
    "def cov_common_to_total_cov(cov_common,data_x):\n",
    "    '''this is just to check for rounding errors. should be equiv to corr_to_total_cov'''\n",
    "    cov = cov_common.copy() \n",
    "    np.fill_diagonal(cov,np.diagonal(np.cov(data_x.T)))\n",
    "    return cov\n",
    "   \n",
    "   \n",
    "def performance(model_candidate,data_fit,data_comp):\n",
    "    model_candidate.fit(data_fit)\n",
    "    candidate_factor_covar = model_factor_cov_from_transform(model_candidate,data_fit)\n",
    "    candidate_cov_common = model_to_cov_common(model_candidate,candidate_factor_covar)\n",
    "    candidate_stdev_marginals = sample_to_stdev_marginals(data_fit)\n",
    "    candidate_corr = cov_common_to_corr(candidate_cov_common,candidate_stdev_marginals)\n",
    "    candidate_cov = corr_to_total_cov(candidate_corr,candidate_stdev_marginals)\n",
    "    candidate_cov_v_sample_cov = np.abs(candidate_cov - np.cov(data_comp.T)).sum()\n",
    "    return { 'factors':model_candidate.n_components_,\n",
    "            'cov_tad':candidate_cov_v_sample_cov,\n",
    "            'cov_tad_pct': candidate_cov_v_sample_cov / np.cov(data_comp.T).sum(),                \n",
    "             'model': model_log_like_base(candidate_cov,\n",
    "                                         model_candidate.mean_,\n",
    "                                         model_candidate.n_features_,\n",
    "                                         data_comp).mean(),\n",
    "            'perfect_marginal_foresight': model_log_like_perfect_marginals(candidate_corr,\n",
    "                                                                           model_candidate.mean_,\n",
    "                                                                           model_candidate.n_features_,\n",
    "                                                                           data_comp).mean(),\n",
    "           'perfect_corr_foresight':model_log_like_perfect_corr(candidate_stdev_marginals,\n",
    "                                                  model_candidate.mean_,\n",
    "                                                  model_candidate.n_features_,\n",
    "                                                  data_comp).mean()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing/Performance Metrics\n",
    "\n",
    "The end goal is to be able to forecast the covariance matrix (for optimization or simulation)\n",
    "I would like to know what you think of the following 'log-liklihood' lifted from the score\n",
    "function the score function in the scikitlearn pca model.\n",
    "\n",
    "I think this is starting to move in the right direction as far as test metrics go, but still want to find a way to separate the correl error from the marginal var error.\n",
    "\n",
    "This is supposed to follow the paper here, which I need to read and come back\n",
    "http://www.miketipping.com/papers/met-mppca.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_log_like_base(model_cov,model_mean,model_n_features,data_x):\n",
    "    '''this is lifted from PCA.score(X)\n",
    "    \n",
    "    this is meant to follow the paper linked here:\n",
    "     https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/decomposition/pca.py\n",
    "    \n",
    "    I copied this out here because I'm having some issue with the PCA classes covar\n",
    "    and the covar I build myself. Also I want to check variations (what do you think??) \n",
    "    '''\n",
    "    precision = linalg.inv(model_cov)\n",
    "    Xr = data_x - model_mean\n",
    "    log_like = -.5 * (Xr * (np.dot(Xr, precision))).sum(axis=1)\n",
    "    log_like -= .5 * (model_n_features * log(2. * np.pi) - fast_logdet(precision))\n",
    "    return log_like\n",
    "\n",
    "#very curious what you think of the below two functions \n",
    "#maybe this a good way to separate the effects of correl and marginal var forecast errors?\n",
    "\n",
    "def model_log_like_perfect_marginals(model_corr,model_mean,model_n_features,data_x):\n",
    "    # purposely using the sample covar for marginals - hence the name 'perfect marginals'\n",
    "    stdev_sample_marginals = np.sqrt(np.diagonal(data_x.cov()))\n",
    "    # reconstruct the cov matrix\n",
    "    model_cov2 = model_corr * np.outer(stdev_sample_marginals,stdev_sample_marginals)\n",
    "    return(model_log_like_base(model_cov2,model_mean,model_n_features,data_x))\n",
    "\n",
    "\n",
    "def model_log_like_perfect_corr(model_stdev_marginals,model_mean,model_n_features,data_x):\n",
    "    # reconstruct the cov matrix\n",
    "    sample_corr = data_x.corr()\n",
    "    model_cov2 =  corr_to_total_cov(sample_corr,model_stdev_marginals)\n",
    "    return(model_log_like_base(model_cov2,model_mean,model_n_features,data_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking SciKitLearn's PCA implementation and testing for stability\n",
    "\n",
    "Basically exploring the differences in ScikitLearn's PCA package and my hand coded function\n",
    "\n",
    "I do not really like the PCA class's get_covariance() function. It looks like it returns the common part only\n",
    "so will use either corr_to_total_cov or cov_common_to_total_cov above.\n",
    "\n",
    "Note: they are following the paper mentioned above in the log-liklihood function which I need to read.\n",
    "    In any case it seems I preserve total var better so will stick to my method.\n",
    "    \n",
    "Note2: there seems to be some instability here. About 1 in 4 times the assert statement below will fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_exp_var': array([4.23872884e-04, 1.05886473e-04, 8.39596749e-05, 6.57204635e-05,\n",
       "        5.08013443e-05]),\n",
       " 'model_exp_var minus noise': array([4.10576517e-04, 9.25901060e-05, 7.06633083e-05, 5.24240969e-05,\n",
       "        3.75049777e-05]),\n",
       " 'manual': array([4.23872884e-04, 1.05886481e-04, 8.39598271e-05, 6.57214254e-05,\n",
       "        5.08073277e-05]),\n",
       " 'mad_cov_common_vs_pca_class_cov': 5.259184742672842e-07,\n",
       " 'mad_pca_class_cov_vs_sample': 8.452016352472613e-07,\n",
       " 'mad_cov_total1_vs_sample_cov': 7.896487985615336e-07,\n",
       " 'mad_cov_total2_vs_sample_cov': 7.894159190984045e-07,\n",
       " 'mad_cov_total1_vs_cov_total2': 2.328794631291389e-10}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the above utils to make sure approximately equal with \n",
    "# SciKitLearns internals\n",
    "\n",
    "#set precision to the 1bps level for var/covar\n",
    "var_atol = 1e-8\n",
    "model_to_test = PCA(n_components=5)\n",
    "model_to_test.fit(data_train)\n",
    "\n",
    "\n",
    "\n",
    "# from PCA class\n",
    "pca_class_factor_covar = model_to_test.explained_variance_ * np.eye(model_to_test.n_components_)\n",
    "pca_class_cov = model_to_test.get_covariance()\n",
    "\n",
    "\n",
    "# model_factor_covar\n",
    "factor_covar = model_factor_cov_from_transform(model_to_test,data_train)\n",
    "# this fails 1 in 4 times or so on my laptop\n",
    "assert np.isclose(pca_class_factor_covar,factor_covar,atol=var_atol).all()\n",
    "\n",
    "\n",
    "# model_to_covar_common\n",
    "cov_common = model_to_cov_common(model_to_test,factor_covar)\n",
    "mad_cov_common_vs_pca_class_cov = np.abs(cov_common - pca_class_cov).mean()\n",
    "\n",
    "\n",
    "# cov_common_to_total_cov\n",
    "stdev_marginals = sample_to_stdev_marginals(data_train)\n",
    "corr = cov_common_to_corr(cov_common,stdev_marginals)\n",
    "cov_total1 = corr_to_total_cov(corr,stdev_marginals)\n",
    "cov_total2 = cov_common_to_total_cov(cov_common,data_train)\n",
    "sample_cov = np.cov(data_train.T)\n",
    "mad_pca_class_cov_vs_sample = np.abs(pca_class_cov - sample_cov).mean()\n",
    "mad_cov_total1_vs_sample_cov = np.abs(cov_total1 - sample_cov).mean()\n",
    "mad_cov_total2_vs_sample_cov = np.abs(cov_total2 - sample_cov).mean()\n",
    "mad_cov_total1_vs_cov_total2 = np.abs(cov_total1 - cov_total2).mean()\n",
    "\n",
    "#\n",
    "\n",
    "\n",
    "#inspection\n",
    "{ 'model_exp_var':model_to_test.explained_variance_,\n",
    "  'model_exp_var minus noise':model_to_test.explained_variance_ - model_to_test.noise_variance_,\n",
    "  'manual':np.diagonal(factor_covar),\n",
    "  'mad_cov_common_vs_pca_class_cov':mad_cov_common_vs_pca_class_cov,\n",
    " #\n",
    "  'mad_pca_class_cov_vs_sample':mad_pca_class_cov_vs_sample,\n",
    "  'mad_cov_total1_vs_sample_cov':mad_cov_total1_vs_sample_cov,\n",
    "  'mad_cov_total2_vs_sample_cov':mad_cov_total2_vs_sample_cov,\n",
    "  'mad_cov_total1_vs_cov_total2':mad_cov_total1_vs_cov_total2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/Validate\n",
    "\n",
    "1st step is determining the number of factors to use with train & valid\n",
    "\n",
    "todo: come back to this to determine the best way to recreate train,valid, and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_generator(data_train,data_valid):\n",
    "    '''model generator. make model on train with different params and validate'''\n",
    "    max_factors = np.linalg.matrix_rank(data_train)\n",
    "    for i in range(max_factors):\n",
    "        n_factors = i + 1\n",
    "        model_candidate = PCA(n_components=n_factors)\n",
    "        model_candidate.fit(data_train)\n",
    "        yield performance(model_candidate,data_train,data_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cov_tad</th>\n",
       "      <th>cov_tad_pct</th>\n",
       "      <th>factors</th>\n",
       "      <th>model</th>\n",
       "      <th>perfect_corr_foresight</th>\n",
       "      <th>perfect_marginal_foresight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.048629e-03</td>\n",
       "      <td>0.167540</td>\n",
       "      <td>1</td>\n",
       "      <td>208.726272</td>\n",
       "      <td>212.234807</td>\n",
       "      <td>208.726113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.428187e-03</td>\n",
       "      <td>0.133443</td>\n",
       "      <td>2</td>\n",
       "      <td>209.102690</td>\n",
       "      <td>212.234807</td>\n",
       "      <td>209.102867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.238854e-03</td>\n",
       "      <td>0.123038</td>\n",
       "      <td>3</td>\n",
       "      <td>209.165866</td>\n",
       "      <td>212.234807</td>\n",
       "      <td>209.166151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.066990e-03</td>\n",
       "      <td>0.113593</td>\n",
       "      <td>4</td>\n",
       "      <td>209.147574</td>\n",
       "      <td>212.234807</td>\n",
       "      <td>209.148030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.975414e-03</td>\n",
       "      <td>0.108560</td>\n",
       "      <td>5</td>\n",
       "      <td>209.209360</td>\n",
       "      <td>212.234807</td>\n",
       "      <td>209.209993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.911142e-03</td>\n",
       "      <td>0.105028</td>\n",
       "      <td>6</td>\n",
       "      <td>209.296251</td>\n",
       "      <td>212.234807</td>\n",
       "      <td>209.296987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.809784e-03</td>\n",
       "      <td>0.099458</td>\n",
       "      <td>7</td>\n",
       "      <td>209.444892</td>\n",
       "      <td>212.234807</td>\n",
       "      <td>209.445603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.698171e-03</td>\n",
       "      <td>0.093324</td>\n",
       "      <td>8</td>\n",
       "      <td>209.455089</td>\n",
       "      <td>212.234807</td>\n",
       "      <td>209.455970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.627200e-03</td>\n",
       "      <td>0.089424</td>\n",
       "      <td>9</td>\n",
       "      <td>209.469324</td>\n",
       "      <td>212.234807</td>\n",
       "      <td>209.470327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.483461e-03</td>\n",
       "      <td>0.081525</td>\n",
       "      <td>10</td>\n",
       "      <td>209.689041</td>\n",
       "      <td>212.234807</td>\n",
       "      <td>209.690167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.339678e-03</td>\n",
       "      <td>0.073623</td>\n",
       "      <td>11</td>\n",
       "      <td>209.701172</td>\n",
       "      <td>212.234807</td>\n",
       "      <td>209.702408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.213231e-03</td>\n",
       "      <td>0.066674</td>\n",
       "      <td>12</td>\n",
       "      <td>209.878257</td>\n",
       "      <td>212.234807</td>\n",
       "      <td>209.879586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.098636e-03</td>\n",
       "      <td>0.060376</td>\n",
       "      <td>13</td>\n",
       "      <td>210.075762</td>\n",
       "      <td>212.234807</td>\n",
       "      <td>210.077049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.019228e-03</td>\n",
       "      <td>0.056012</td>\n",
       "      <td>14</td>\n",
       "      <td>210.032876</td>\n",
       "      <td>212.234807</td>\n",
       "      <td>210.034285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>9.651978e-04</td>\n",
       "      <td>0.053043</td>\n",
       "      <td>15</td>\n",
       "      <td>210.019595</td>\n",
       "      <td>212.234807</td>\n",
       "      <td>210.021029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>8.852889e-04</td>\n",
       "      <td>0.048652</td>\n",
       "      <td>16</td>\n",
       "      <td>210.109807</td>\n",
       "      <td>212.234807</td>\n",
       "      <td>210.111249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>8.029151e-04</td>\n",
       "      <td>0.044125</td>\n",
       "      <td>17</td>\n",
       "      <td>210.377523</td>\n",
       "      <td>212.234807</td>\n",
       "      <td>210.378920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>7.592617e-04</td>\n",
       "      <td>0.041726</td>\n",
       "      <td>18</td>\n",
       "      <td>210.317724</td>\n",
       "      <td>212.234807</td>\n",
       "      <td>210.319194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>7.034005e-04</td>\n",
       "      <td>0.038656</td>\n",
       "      <td>19</td>\n",
       "      <td>210.263845</td>\n",
       "      <td>212.234807</td>\n",
       "      <td>210.265402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>6.645782e-04</td>\n",
       "      <td>0.036522</td>\n",
       "      <td>20</td>\n",
       "      <td>210.324859</td>\n",
       "      <td>212.234807</td>\n",
       "      <td>210.326425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>6.544778e-04</td>\n",
       "      <td>0.035967</td>\n",
       "      <td>21</td>\n",
       "      <td>210.209808</td>\n",
       "      <td>212.234807</td>\n",
       "      <td>210.211477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>6.125083e-04</td>\n",
       "      <td>0.033661</td>\n",
       "      <td>22</td>\n",
       "      <td>210.456029</td>\n",
       "      <td>212.234807</td>\n",
       "      <td>210.457539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>5.824708e-04</td>\n",
       "      <td>0.032010</td>\n",
       "      <td>23</td>\n",
       "      <td>210.565175</td>\n",
       "      <td>212.234807</td>\n",
       "      <td>210.566673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>5.606016e-04</td>\n",
       "      <td>0.030808</td>\n",
       "      <td>24</td>\n",
       "      <td>210.493177</td>\n",
       "      <td>212.234807</td>\n",
       "      <td>210.494748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>5.321429e-04</td>\n",
       "      <td>0.029244</td>\n",
       "      <td>25</td>\n",
       "      <td>210.328999</td>\n",
       "      <td>212.234807</td>\n",
       "      <td>210.330669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5.129886e-04</td>\n",
       "      <td>0.028192</td>\n",
       "      <td>26</td>\n",
       "      <td>210.273847</td>\n",
       "      <td>212.234807</td>\n",
       "      <td>210.275642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>4.965528e-04</td>\n",
       "      <td>0.027288</td>\n",
       "      <td>27</td>\n",
       "      <td>210.151807</td>\n",
       "      <td>212.234807</td>\n",
       "      <td>210.153727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4.643121e-04</td>\n",
       "      <td>0.025517</td>\n",
       "      <td>28</td>\n",
       "      <td>210.175305</td>\n",
       "      <td>212.234807</td>\n",
       "      <td>210.177230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>4.382806e-04</td>\n",
       "      <td>0.024086</td>\n",
       "      <td>29</td>\n",
       "      <td>209.974526</td>\n",
       "      <td>212.234807</td>\n",
       "      <td>209.976571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4.296750e-04</td>\n",
       "      <td>0.023613</td>\n",
       "      <td>30</td>\n",
       "      <td>209.883441</td>\n",
       "      <td>212.234807</td>\n",
       "      <td>209.885554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>4.142285e-04</td>\n",
       "      <td>0.022764</td>\n",
       "      <td>31</td>\n",
       "      <td>209.862175</td>\n",
       "      <td>212.234807</td>\n",
       "      <td>209.864310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>3.732913e-04</td>\n",
       "      <td>0.020514</td>\n",
       "      <td>32</td>\n",
       "      <td>210.118888</td>\n",
       "      <td>212.234807</td>\n",
       "      <td>210.120848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>3.583140e-04</td>\n",
       "      <td>0.019691</td>\n",
       "      <td>33</td>\n",
       "      <td>210.127163</td>\n",
       "      <td>212.234807</td>\n",
       "      <td>210.129125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>3.436407e-04</td>\n",
       "      <td>0.018885</td>\n",
       "      <td>34</td>\n",
       "      <td>209.990597</td>\n",
       "      <td>212.234807</td>\n",
       "      <td>209.992640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>3.226931e-04</td>\n",
       "      <td>0.017734</td>\n",
       "      <td>35</td>\n",
       "      <td>209.836682</td>\n",
       "      <td>212.234807</td>\n",
       "      <td>209.838819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2.957156e-04</td>\n",
       "      <td>0.016251</td>\n",
       "      <td>36</td>\n",
       "      <td>209.905813</td>\n",
       "      <td>212.234807</td>\n",
       "      <td>209.907877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2.709885e-04</td>\n",
       "      <td>0.014892</td>\n",
       "      <td>37</td>\n",
       "      <td>209.881867</td>\n",
       "      <td>212.234807</td>\n",
       "      <td>209.883943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2.540961e-04</td>\n",
       "      <td>0.013964</td>\n",
       "      <td>38</td>\n",
       "      <td>209.737706</td>\n",
       "      <td>212.234807</td>\n",
       "      <td>209.739839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2.344979e-04</td>\n",
       "      <td>0.012887</td>\n",
       "      <td>39</td>\n",
       "      <td>209.551698</td>\n",
       "      <td>212.234807</td>\n",
       "      <td>209.553900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2.085194e-04</td>\n",
       "      <td>0.011459</td>\n",
       "      <td>40</td>\n",
       "      <td>210.056209</td>\n",
       "      <td>212.234807</td>\n",
       "      <td>210.058051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1.849724e-04</td>\n",
       "      <td>0.010165</td>\n",
       "      <td>41</td>\n",
       "      <td>210.315581</td>\n",
       "      <td>212.234807</td>\n",
       "      <td>210.317217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1.639909e-04</td>\n",
       "      <td>0.009012</td>\n",
       "      <td>42</td>\n",
       "      <td>210.287183</td>\n",
       "      <td>212.234807</td>\n",
       "      <td>210.288786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1.242680e-04</td>\n",
       "      <td>0.006829</td>\n",
       "      <td>43</td>\n",
       "      <td>211.189662</td>\n",
       "      <td>212.234807</td>\n",
       "      <td>211.190702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1.081439e-04</td>\n",
       "      <td>0.005943</td>\n",
       "      <td>44</td>\n",
       "      <td>211.063813</td>\n",
       "      <td>212.234807</td>\n",
       "      <td>211.064896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>9.199280e-05</td>\n",
       "      <td>0.005056</td>\n",
       "      <td>45</td>\n",
       "      <td>210.891779</td>\n",
       "      <td>212.234807</td>\n",
       "      <td>210.892944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>7.458358e-05</td>\n",
       "      <td>0.004099</td>\n",
       "      <td>46</td>\n",
       "      <td>210.890912</td>\n",
       "      <td>212.234807</td>\n",
       "      <td>210.892023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>6.037133e-05</td>\n",
       "      <td>0.003318</td>\n",
       "      <td>47</td>\n",
       "      <td>210.732314</td>\n",
       "      <td>212.234807</td>\n",
       "      <td>210.733429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>4.163755e-05</td>\n",
       "      <td>0.002288</td>\n",
       "      <td>48</td>\n",
       "      <td>211.359708</td>\n",
       "      <td>212.234807</td>\n",
       "      <td>211.360385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2.387772e-05</td>\n",
       "      <td>0.001312</td>\n",
       "      <td>49</td>\n",
       "      <td>211.988378</td>\n",
       "      <td>212.234807</td>\n",
       "      <td>211.988601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>5.821987e-07</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>50</td>\n",
       "      <td>212.234802</td>\n",
       "      <td>212.234807</td>\n",
       "      <td>212.234805</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         cov_tad  cov_tad_pct  factors       model  perfect_corr_foresight  \\\n",
       "0   3.048629e-03     0.167540        1  208.726272              212.234807   \n",
       "1   2.428187e-03     0.133443        2  209.102690              212.234807   \n",
       "2   2.238854e-03     0.123038        3  209.165866              212.234807   \n",
       "3   2.066990e-03     0.113593        4  209.147574              212.234807   \n",
       "4   1.975414e-03     0.108560        5  209.209360              212.234807   \n",
       "5   1.911142e-03     0.105028        6  209.296251              212.234807   \n",
       "6   1.809784e-03     0.099458        7  209.444892              212.234807   \n",
       "7   1.698171e-03     0.093324        8  209.455089              212.234807   \n",
       "8   1.627200e-03     0.089424        9  209.469324              212.234807   \n",
       "9   1.483461e-03     0.081525       10  209.689041              212.234807   \n",
       "10  1.339678e-03     0.073623       11  209.701172              212.234807   \n",
       "11  1.213231e-03     0.066674       12  209.878257              212.234807   \n",
       "12  1.098636e-03     0.060376       13  210.075762              212.234807   \n",
       "13  1.019228e-03     0.056012       14  210.032876              212.234807   \n",
       "14  9.651978e-04     0.053043       15  210.019595              212.234807   \n",
       "15  8.852889e-04     0.048652       16  210.109807              212.234807   \n",
       "16  8.029151e-04     0.044125       17  210.377523              212.234807   \n",
       "17  7.592617e-04     0.041726       18  210.317724              212.234807   \n",
       "18  7.034005e-04     0.038656       19  210.263845              212.234807   \n",
       "19  6.645782e-04     0.036522       20  210.324859              212.234807   \n",
       "20  6.544778e-04     0.035967       21  210.209808              212.234807   \n",
       "21  6.125083e-04     0.033661       22  210.456029              212.234807   \n",
       "22  5.824708e-04     0.032010       23  210.565175              212.234807   \n",
       "23  5.606016e-04     0.030808       24  210.493177              212.234807   \n",
       "24  5.321429e-04     0.029244       25  210.328999              212.234807   \n",
       "25  5.129886e-04     0.028192       26  210.273847              212.234807   \n",
       "26  4.965528e-04     0.027288       27  210.151807              212.234807   \n",
       "27  4.643121e-04     0.025517       28  210.175305              212.234807   \n",
       "28  4.382806e-04     0.024086       29  209.974526              212.234807   \n",
       "29  4.296750e-04     0.023613       30  209.883441              212.234807   \n",
       "30  4.142285e-04     0.022764       31  209.862175              212.234807   \n",
       "31  3.732913e-04     0.020514       32  210.118888              212.234807   \n",
       "32  3.583140e-04     0.019691       33  210.127163              212.234807   \n",
       "33  3.436407e-04     0.018885       34  209.990597              212.234807   \n",
       "34  3.226931e-04     0.017734       35  209.836682              212.234807   \n",
       "35  2.957156e-04     0.016251       36  209.905813              212.234807   \n",
       "36  2.709885e-04     0.014892       37  209.881867              212.234807   \n",
       "37  2.540961e-04     0.013964       38  209.737706              212.234807   \n",
       "38  2.344979e-04     0.012887       39  209.551698              212.234807   \n",
       "39  2.085194e-04     0.011459       40  210.056209              212.234807   \n",
       "40  1.849724e-04     0.010165       41  210.315581              212.234807   \n",
       "41  1.639909e-04     0.009012       42  210.287183              212.234807   \n",
       "42  1.242680e-04     0.006829       43  211.189662              212.234807   \n",
       "43  1.081439e-04     0.005943       44  211.063813              212.234807   \n",
       "44  9.199280e-05     0.005056       45  210.891779              212.234807   \n",
       "45  7.458358e-05     0.004099       46  210.890912              212.234807   \n",
       "46  6.037133e-05     0.003318       47  210.732314              212.234807   \n",
       "47  4.163755e-05     0.002288       48  211.359708              212.234807   \n",
       "48  2.387772e-05     0.001312       49  211.988378              212.234807   \n",
       "49  5.821987e-07     0.000032       50  212.234802              212.234807   \n",
       "\n",
       "    perfect_marginal_foresight  \n",
       "0                   208.726113  \n",
       "1                   209.102867  \n",
       "2                   209.166151  \n",
       "3                   209.148030  \n",
       "4                   209.209993  \n",
       "5                   209.296987  \n",
       "6                   209.445603  \n",
       "7                   209.455970  \n",
       "8                   209.470327  \n",
       "9                   209.690167  \n",
       "10                  209.702408  \n",
       "11                  209.879586  \n",
       "12                  210.077049  \n",
       "13                  210.034285  \n",
       "14                  210.021029  \n",
       "15                  210.111249  \n",
       "16                  210.378920  \n",
       "17                  210.319194  \n",
       "18                  210.265402  \n",
       "19                  210.326425  \n",
       "20                  210.211477  \n",
       "21                  210.457539  \n",
       "22                  210.566673  \n",
       "23                  210.494748  \n",
       "24                  210.330669  \n",
       "25                  210.275642  \n",
       "26                  210.153727  \n",
       "27                  210.177230  \n",
       "28                  209.976571  \n",
       "29                  209.885554  \n",
       "30                  209.864310  \n",
       "31                  210.120848  \n",
       "32                  210.129125  \n",
       "33                  209.992640  \n",
       "34                  209.838819  \n",
       "35                  209.907877  \n",
       "36                  209.883943  \n",
       "37                  209.739839  \n",
       "38                  209.553900  \n",
       "39                  210.058051  \n",
       "40                  210.317217  \n",
       "41                  210.288786  \n",
       "42                  211.190702  \n",
       "43                  211.064896  \n",
       "44                  210.892944  \n",
       "45                  210.892023  \n",
       "46                  210.733429  \n",
       "47                  211.360385  \n",
       "48                  211.988601  \n",
       "49                  212.234805  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is just a sanity check\n",
    "# when the train and valid are the same data,\n",
    "# by design 'model' should be the same as 'perfect_marginal_foresight' (but we have decent rounding errors)\n",
    "# and 'perfect_corr_foresight' should be a constant = the 50 factor (full rank model)\n",
    "# the cov_tad should decrease to zero for the 50 factor model, and so shoudl cov_tad_pct\n",
    "pd.DataFrame(pca_generator(data_train,data_train))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cov_tad</th>\n",
       "      <th>cov_tad_pct</th>\n",
       "      <th>factors</th>\n",
       "      <th>model</th>\n",
       "      <th>perfect_corr_foresight</th>\n",
       "      <th>perfect_marginal_foresight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.018184</td>\n",
       "      <td>0.495022</td>\n",
       "      <td>1</td>\n",
       "      <td>200.415863</td>\n",
       "      <td>203.620773</td>\n",
       "      <td>202.805759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.018107</td>\n",
       "      <td>0.492908</td>\n",
       "      <td>2</td>\n",
       "      <td>200.211997</td>\n",
       "      <td>203.620773</td>\n",
       "      <td>203.043128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.018157</td>\n",
       "      <td>0.494274</td>\n",
       "      <td>3</td>\n",
       "      <td>199.886114</td>\n",
       "      <td>203.620773</td>\n",
       "      <td>202.851501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.018239</td>\n",
       "      <td>0.496504</td>\n",
       "      <td>4</td>\n",
       "      <td>199.784350</td>\n",
       "      <td>203.620773</td>\n",
       "      <td>202.893440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.018293</td>\n",
       "      <td>0.497990</td>\n",
       "      <td>5</td>\n",
       "      <td>199.603053</td>\n",
       "      <td>203.620773</td>\n",
       "      <td>202.877066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.018355</td>\n",
       "      <td>0.499664</td>\n",
       "      <td>6</td>\n",
       "      <td>199.052828</td>\n",
       "      <td>203.620773</td>\n",
       "      <td>202.499796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.018389</td>\n",
       "      <td>0.500586</td>\n",
       "      <td>7</td>\n",
       "      <td>198.884897</td>\n",
       "      <td>203.620773</td>\n",
       "      <td>202.540199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.018431</td>\n",
       "      <td>0.501743</td>\n",
       "      <td>8</td>\n",
       "      <td>198.407114</td>\n",
       "      <td>203.620773</td>\n",
       "      <td>202.431232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.018462</td>\n",
       "      <td>0.502584</td>\n",
       "      <td>9</td>\n",
       "      <td>198.496151</td>\n",
       "      <td>203.620773</td>\n",
       "      <td>202.539061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.018442</td>\n",
       "      <td>0.502030</td>\n",
       "      <td>10</td>\n",
       "      <td>198.115720</td>\n",
       "      <td>203.620773</td>\n",
       "      <td>202.444598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.018468</td>\n",
       "      <td>0.502737</td>\n",
       "      <td>11</td>\n",
       "      <td>197.491060</td>\n",
       "      <td>203.620773</td>\n",
       "      <td>202.158017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.018488</td>\n",
       "      <td>0.503291</td>\n",
       "      <td>12</td>\n",
       "      <td>197.522147</td>\n",
       "      <td>203.620773</td>\n",
       "      <td>202.225140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.018495</td>\n",
       "      <td>0.503480</td>\n",
       "      <td>13</td>\n",
       "      <td>197.438397</td>\n",
       "      <td>203.620773</td>\n",
       "      <td>202.292015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.018513</td>\n",
       "      <td>0.503971</td>\n",
       "      <td>14</td>\n",
       "      <td>196.704789</td>\n",
       "      <td>203.620773</td>\n",
       "      <td>202.087647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.018517</td>\n",
       "      <td>0.504070</td>\n",
       "      <td>15</td>\n",
       "      <td>196.613996</td>\n",
       "      <td>203.620773</td>\n",
       "      <td>202.112072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.018545</td>\n",
       "      <td>0.504841</td>\n",
       "      <td>16</td>\n",
       "      <td>195.759414</td>\n",
       "      <td>203.620773</td>\n",
       "      <td>201.856612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.018563</td>\n",
       "      <td>0.505335</td>\n",
       "      <td>17</td>\n",
       "      <td>196.563519</td>\n",
       "      <td>203.620773</td>\n",
       "      <td>202.208165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.018564</td>\n",
       "      <td>0.505360</td>\n",
       "      <td>18</td>\n",
       "      <td>196.455205</td>\n",
       "      <td>203.620773</td>\n",
       "      <td>202.163865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.018565</td>\n",
       "      <td>0.505380</td>\n",
       "      <td>19</td>\n",
       "      <td>196.413623</td>\n",
       "      <td>203.620773</td>\n",
       "      <td>202.085523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.018570</td>\n",
       "      <td>0.505525</td>\n",
       "      <td>20</td>\n",
       "      <td>196.193051</td>\n",
       "      <td>203.620773</td>\n",
       "      <td>202.049288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.018577</td>\n",
       "      <td>0.505707</td>\n",
       "      <td>21</td>\n",
       "      <td>195.637464</td>\n",
       "      <td>203.620773</td>\n",
       "      <td>201.774815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.018589</td>\n",
       "      <td>0.506048</td>\n",
       "      <td>22</td>\n",
       "      <td>195.981195</td>\n",
       "      <td>203.620773</td>\n",
       "      <td>201.877204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.018598</td>\n",
       "      <td>0.506282</td>\n",
       "      <td>23</td>\n",
       "      <td>196.370498</td>\n",
       "      <td>203.620773</td>\n",
       "      <td>202.102945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.018608</td>\n",
       "      <td>0.506566</td>\n",
       "      <td>24</td>\n",
       "      <td>195.877321</td>\n",
       "      <td>203.620773</td>\n",
       "      <td>201.841709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.018620</td>\n",
       "      <td>0.506896</td>\n",
       "      <td>25</td>\n",
       "      <td>195.168919</td>\n",
       "      <td>203.620773</td>\n",
       "      <td>201.474222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.018632</td>\n",
       "      <td>0.507214</td>\n",
       "      <td>26</td>\n",
       "      <td>194.836867</td>\n",
       "      <td>203.620773</td>\n",
       "      <td>201.308162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.018640</td>\n",
       "      <td>0.507438</td>\n",
       "      <td>27</td>\n",
       "      <td>194.623823</td>\n",
       "      <td>203.620773</td>\n",
       "      <td>201.193082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.018648</td>\n",
       "      <td>0.507655</td>\n",
       "      <td>28</td>\n",
       "      <td>194.505668</td>\n",
       "      <td>203.620773</td>\n",
       "      <td>201.198043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.018655</td>\n",
       "      <td>0.507838</td>\n",
       "      <td>29</td>\n",
       "      <td>194.188645</td>\n",
       "      <td>203.620773</td>\n",
       "      <td>200.993042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.018655</td>\n",
       "      <td>0.507834</td>\n",
       "      <td>30</td>\n",
       "      <td>193.776030</td>\n",
       "      <td>203.620773</td>\n",
       "      <td>200.791404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.018662</td>\n",
       "      <td>0.508017</td>\n",
       "      <td>31</td>\n",
       "      <td>193.649069</td>\n",
       "      <td>203.620773</td>\n",
       "      <td>200.671345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.018666</td>\n",
       "      <td>0.508134</td>\n",
       "      <td>32</td>\n",
       "      <td>194.295039</td>\n",
       "      <td>203.620773</td>\n",
       "      <td>201.055049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.018672</td>\n",
       "      <td>0.508301</td>\n",
       "      <td>33</td>\n",
       "      <td>193.699690</td>\n",
       "      <td>203.620773</td>\n",
       "      <td>200.631233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.018675</td>\n",
       "      <td>0.508377</td>\n",
       "      <td>34</td>\n",
       "      <td>193.056093</td>\n",
       "      <td>203.620773</td>\n",
       "      <td>200.225756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.018682</td>\n",
       "      <td>0.508564</td>\n",
       "      <td>35</td>\n",
       "      <td>192.822611</td>\n",
       "      <td>203.620773</td>\n",
       "      <td>200.028273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.018685</td>\n",
       "      <td>0.508648</td>\n",
       "      <td>36</td>\n",
       "      <td>192.919799</td>\n",
       "      <td>203.620773</td>\n",
       "      <td>200.056506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.018690</td>\n",
       "      <td>0.508792</td>\n",
       "      <td>37</td>\n",
       "      <td>192.746304</td>\n",
       "      <td>203.620773</td>\n",
       "      <td>199.923228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.018696</td>\n",
       "      <td>0.508958</td>\n",
       "      <td>38</td>\n",
       "      <td>192.372738</td>\n",
       "      <td>203.620773</td>\n",
       "      <td>199.690461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.018700</td>\n",
       "      <td>0.509063</td>\n",
       "      <td>39</td>\n",
       "      <td>191.939140</td>\n",
       "      <td>203.620773</td>\n",
       "      <td>199.500727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.018703</td>\n",
       "      <td>0.509138</td>\n",
       "      <td>40</td>\n",
       "      <td>192.945407</td>\n",
       "      <td>203.620773</td>\n",
       "      <td>200.089889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.018709</td>\n",
       "      <td>0.509306</td>\n",
       "      <td>41</td>\n",
       "      <td>193.394367</td>\n",
       "      <td>203.620773</td>\n",
       "      <td>200.322484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.018713</td>\n",
       "      <td>0.509421</td>\n",
       "      <td>42</td>\n",
       "      <td>193.282962</td>\n",
       "      <td>203.620773</td>\n",
       "      <td>200.230453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.018717</td>\n",
       "      <td>0.509536</td>\n",
       "      <td>43</td>\n",
       "      <td>195.856244</td>\n",
       "      <td>203.620773</td>\n",
       "      <td>201.719636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.018720</td>\n",
       "      <td>0.509612</td>\n",
       "      <td>44</td>\n",
       "      <td>195.745303</td>\n",
       "      <td>203.620773</td>\n",
       "      <td>201.682907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.018721</td>\n",
       "      <td>0.509639</td>\n",
       "      <td>45</td>\n",
       "      <td>195.537929</td>\n",
       "      <td>203.620773</td>\n",
       "      <td>201.529956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.018721</td>\n",
       "      <td>0.509631</td>\n",
       "      <td>46</td>\n",
       "      <td>195.557252</td>\n",
       "      <td>203.620773</td>\n",
       "      <td>201.573405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.018723</td>\n",
       "      <td>0.509679</td>\n",
       "      <td>47</td>\n",
       "      <td>194.917906</td>\n",
       "      <td>203.620773</td>\n",
       "      <td>201.063943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.018723</td>\n",
       "      <td>0.509684</td>\n",
       "      <td>48</td>\n",
       "      <td>197.058592</td>\n",
       "      <td>203.620773</td>\n",
       "      <td>202.146452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.018724</td>\n",
       "      <td>0.509705</td>\n",
       "      <td>49</td>\n",
       "      <td>198.785171</td>\n",
       "      <td>203.620773</td>\n",
       "      <td>203.062125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.018725</td>\n",
       "      <td>0.509745</td>\n",
       "      <td>50</td>\n",
       "      <td>199.784921</td>\n",
       "      <td>203.620773</td>\n",
       "      <td>203.595514</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     cov_tad  cov_tad_pct  factors       model  perfect_corr_foresight  \\\n",
       "0   0.018184     0.495022        1  200.415863              203.620773   \n",
       "1   0.018107     0.492908        2  200.211997              203.620773   \n",
       "2   0.018157     0.494274        3  199.886114              203.620773   \n",
       "3   0.018239     0.496504        4  199.784350              203.620773   \n",
       "4   0.018293     0.497990        5  199.603053              203.620773   \n",
       "5   0.018355     0.499664        6  199.052828              203.620773   \n",
       "6   0.018389     0.500586        7  198.884897              203.620773   \n",
       "7   0.018431     0.501743        8  198.407114              203.620773   \n",
       "8   0.018462     0.502584        9  198.496151              203.620773   \n",
       "9   0.018442     0.502030       10  198.115720              203.620773   \n",
       "10  0.018468     0.502737       11  197.491060              203.620773   \n",
       "11  0.018488     0.503291       12  197.522147              203.620773   \n",
       "12  0.018495     0.503480       13  197.438397              203.620773   \n",
       "13  0.018513     0.503971       14  196.704789              203.620773   \n",
       "14  0.018517     0.504070       15  196.613996              203.620773   \n",
       "15  0.018545     0.504841       16  195.759414              203.620773   \n",
       "16  0.018563     0.505335       17  196.563519              203.620773   \n",
       "17  0.018564     0.505360       18  196.455205              203.620773   \n",
       "18  0.018565     0.505380       19  196.413623              203.620773   \n",
       "19  0.018570     0.505525       20  196.193051              203.620773   \n",
       "20  0.018577     0.505707       21  195.637464              203.620773   \n",
       "21  0.018589     0.506048       22  195.981195              203.620773   \n",
       "22  0.018598     0.506282       23  196.370498              203.620773   \n",
       "23  0.018608     0.506566       24  195.877321              203.620773   \n",
       "24  0.018620     0.506896       25  195.168919              203.620773   \n",
       "25  0.018632     0.507214       26  194.836867              203.620773   \n",
       "26  0.018640     0.507438       27  194.623823              203.620773   \n",
       "27  0.018648     0.507655       28  194.505668              203.620773   \n",
       "28  0.018655     0.507838       29  194.188645              203.620773   \n",
       "29  0.018655     0.507834       30  193.776030              203.620773   \n",
       "30  0.018662     0.508017       31  193.649069              203.620773   \n",
       "31  0.018666     0.508134       32  194.295039              203.620773   \n",
       "32  0.018672     0.508301       33  193.699690              203.620773   \n",
       "33  0.018675     0.508377       34  193.056093              203.620773   \n",
       "34  0.018682     0.508564       35  192.822611              203.620773   \n",
       "35  0.018685     0.508648       36  192.919799              203.620773   \n",
       "36  0.018690     0.508792       37  192.746304              203.620773   \n",
       "37  0.018696     0.508958       38  192.372738              203.620773   \n",
       "38  0.018700     0.509063       39  191.939140              203.620773   \n",
       "39  0.018703     0.509138       40  192.945407              203.620773   \n",
       "40  0.018709     0.509306       41  193.394367              203.620773   \n",
       "41  0.018713     0.509421       42  193.282962              203.620773   \n",
       "42  0.018717     0.509536       43  195.856244              203.620773   \n",
       "43  0.018720     0.509612       44  195.745303              203.620773   \n",
       "44  0.018721     0.509639       45  195.537929              203.620773   \n",
       "45  0.018721     0.509631       46  195.557252              203.620773   \n",
       "46  0.018723     0.509679       47  194.917906              203.620773   \n",
       "47  0.018723     0.509684       48  197.058592              203.620773   \n",
       "48  0.018724     0.509705       49  198.785171              203.620773   \n",
       "49  0.018725     0.509745       50  199.784921              203.620773   \n",
       "\n",
       "    perfect_marginal_foresight  \n",
       "0                   202.805759  \n",
       "1                   203.043128  \n",
       "2                   202.851501  \n",
       "3                   202.893440  \n",
       "4                   202.877066  \n",
       "5                   202.499796  \n",
       "6                   202.540199  \n",
       "7                   202.431232  \n",
       "8                   202.539061  \n",
       "9                   202.444598  \n",
       "10                  202.158017  \n",
       "11                  202.225140  \n",
       "12                  202.292015  \n",
       "13                  202.087647  \n",
       "14                  202.112072  \n",
       "15                  201.856612  \n",
       "16                  202.208165  \n",
       "17                  202.163865  \n",
       "18                  202.085523  \n",
       "19                  202.049288  \n",
       "20                  201.774815  \n",
       "21                  201.877204  \n",
       "22                  202.102945  \n",
       "23                  201.841709  \n",
       "24                  201.474222  \n",
       "25                  201.308162  \n",
       "26                  201.193082  \n",
       "27                  201.198043  \n",
       "28                  200.993042  \n",
       "29                  200.791404  \n",
       "30                  200.671345  \n",
       "31                  201.055049  \n",
       "32                  200.631233  \n",
       "33                  200.225756  \n",
       "34                  200.028273  \n",
       "35                  200.056506  \n",
       "36                  199.923228  \n",
       "37                  199.690461  \n",
       "38                  199.500727  \n",
       "39                  200.089889  \n",
       "40                  200.322484  \n",
       "41                  200.230453  \n",
       "42                  201.719636  \n",
       "43                  201.682907  \n",
       "44                  201.529956  \n",
       "45                  201.573405  \n",
       "46                  201.063943  \n",
       "47                  202.146452  \n",
       "48                  203.062125  \n",
       "49                  203.595514  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# From this (building the model with data_train and using it on data_valid)\n",
    "# it seems that up to 2 factors makes sense but probably not more\n",
    "# note that this changes over time and context. one of the tricks using\n",
    "# pca is how many factors are appropriate to use on a given day\n",
    "pd.DataFrame(pca_generator(data_train,data_valid))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change benchmark: encoding accuracy to encoding forecast accuracy\n",
    "\n",
    "Assume we can use any data up to the test_data cutoff(s). I'm using data_valid just because its the closest to data_test in time. In practice we can use all the data up to test_data\n",
    "\n",
    "Although I am interested in the sparce encoding of the return space to describe what has happened over the last hour, days, etc (and can use this as input into the forecast), at the end of the day I really care about the future. \n",
    "\n",
    "We are assuming in this PCA model case that we can model the whole market as 5 iid gaussian shocks + 50 individual shocks. I want to test that the covar matrix we produce is the best forecast of the forward covar matrix. And since we can split that into two separate forecasts: the correlations and the marginals, I'm really most interested in the accuracy and stability of the correlation matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'factors': 2,\n",
       " 'cov_tad': 0.013307688613307854,\n",
       " 'cov_tad_pct': 0.515954107558356,\n",
       " 'model': 199.9243424212995,\n",
       " 'perfect_marginal_foresight': 201.24610738292097,\n",
       " 'perfect_corr_foresight': 206.3378506563013}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_FACTORS=2 # now from validation step, see above\n",
    "\n",
    "model0 = PCA(n_components=N_FACTORS) \n",
    "model0 = model0.fit(data_valid) # fitting with data_valid just because its close to train\n",
    "performance(model0,data_valid,data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'factors': nan,\n",
       " 'cov_tad': 0.012989327704128265,\n",
       " 'cov_tad_pct': 0.5036108957843023,\n",
       " 'model': 196.94178608325038,\n",
       " 'perfect_marginal_foresight': 197.0369753735743,\n",
       " 'perfect_corr_foresight': 206.33726972671386}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this should really be a separate model \n",
    "# but lets comp the above to a simple sample covar/correl matrx\n",
    "# also note, this is how most people make their covar matrix - the simple,naiive way!\n",
    "# this should be the same as a 50 factor pca model as currently designed\n",
    "\n",
    "baseline_mean = data_valid.mean()\n",
    "baseline_cov = np.cov(data_valid.T)\n",
    "baseline_corr = data_valid.corr()\n",
    "baseline_stdev_marginals = np.sqrt(np.diagonal(baseline_cov)) # in this case same as pca\n",
    "baseline_features = np.linalg.matrix_rank(data_valid)\n",
    "\n",
    "cov_tad_baseline = np.abs(baseline_cov - np.cov(data_test.T)).sum()\n",
    "{ 'factors':np.nan,\n",
    "  'cov_tad':cov_tad_baseline,\n",
    "  'cov_tad_pct': cov_tad_baseline / np.cov(data_test.T).sum(),                \n",
    "   'model': model_log_like_base(baseline_cov,\n",
    "                                baseline_mean,                                \n",
    "                                baseline_features,\n",
    "                                data_test).mean(),\n",
    " 'perfect_marginal_foresight': model_log_like_perfect_marginals(baseline_corr,\n",
    "                                                                baseline_mean,\n",
    "                                                                baseline_features,\n",
    "                                                                data_test).mean(),\n",
    "'perfect_corr_foresight':model_log_like_perfect_corr(baseline_stdev_marginals,\n",
    "                                                     baseline_mean,\n",
    "                                                     baseline_features,\n",
    "                                                     data_test).mean()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0\n",
       "2018-07-24    204.747167\n",
       "2018-07-25    204.217117\n",
       "2018-07-26    209.331416\n",
       "2018-07-27    206.222979\n",
       "2018-07-30    211.711891\n",
       "2018-07-31    205.355864\n",
       "2018-08-01    200.083791\n",
       "2018-08-02    202.894896\n",
       "2018-08-03    196.971652\n",
       "2018-08-06    180.226590\n",
       "2018-08-07    189.045730\n",
       "2018-08-08    197.505094\n",
       "2018-08-09    194.330078\n",
       "2018-08-10    205.262286\n",
       "2018-08-13    200.737798\n",
       "2018-08-14    192.836520\n",
       "2018-08-15    190.801233\n",
       "2018-08-16    175.269108\n",
       "2018-08-17    192.655482\n",
       "2018-08-20    189.485981\n",
       "2018-08-21    187.929742\n",
       "Freq: B, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# another way to look at log liklihood - over time \n",
    "# going back to train/valid this would be a good way to set up windows maybe\n",
    "\n",
    "baseline_log_like = model_log_like_base(baseline_cov,\n",
    "                                baseline_mean,                                \n",
    "                                baseline_features,\n",
    "                                data_test)\n",
    "\n",
    "baseline_log_like.groupby(pd.Grouper(freq='B')).mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Older stuff - keeping around for now\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA factor covariance matrix \n",
    "cov_factor0 = data_train_encoded.cov()\n",
    "\n",
    "# if the models is iid, the off diagnoal should be close to zero\n",
    "corr_factor0 = data_train_encoded.corr()\n",
    "corr_factor0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: i did it this way (factor_exp * covar_common * factor_exp) for illustration\n",
    "factor_exposures0 = model0.components_\n",
    "cov_from_common_factors0 = pd.DataFrame(np.dot(np.dot(factor_exposures0.T,cov_factor0),factor_exposures0))\n",
    "\n",
    "# note: can also do like below, as you can see, they are equivalent\n",
    "np.isclose(pd.DataFrame(model0.inverse_transform(data_train_encoded)).cov(),cov_from_common_factors0).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#because correlation comes from cross sectional effects only we can create like this (one of many ways):\n",
    "sample_cov_valid = data_valid.cov()\n",
    "var_marginals0 = np.diagonal(sample_cov_valid)\n",
    "stdev_marginals0 = np.sqrt(var_marginals0)\n",
    "corr0 = cov_from_common_factors0 / np.outer(stdev_marginals0,stdev_marginals0)\n",
    "np.fill_diagonal(corr0.values,1.0)\n",
    "cov0 = corr0 * np.outer(stdev_marginals0,stdev_marginals0)\n",
    "\n",
    "#inspect\n",
    "pd.DataFrame({'model':model0.get_covariance()[:,0],\n",
    "              'model_common+sample_marginal':cov0.iloc[:,0],\n",
    "              'data_valid':sample_cov_valid.iloc[:,0].values})\n",
    "\n",
    "#todo: why isnt the model an model_common + sample_marginal closer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can also shave off the specific var for use later as well\n",
    "var_specific0 = np.diagonal(data_valid.cov()) - np.diagonal(cov_from_common_factors0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inspect\n",
    "var_breakdown = pd.DataFrame({'marginal':var_marginals0,\n",
    "                              'specific':var_specific0,\n",
    "                              'pct':var_specific0/var_marginals0}).set_index(data_valid.columns)\n",
    " \n",
    "#this should be empty    \n",
    "var_breakdown[var_breakdown['pct']< 0.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_like0 = model_log_like(cov0,model0.mean_,model0.n_features_,data_test)    \n",
    "log_like0.groupby(pd.Grouper(freq='B')).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this shoudl really be a separate model (and I'll make it one when I have a minute)\n",
    "# but lets comp the above to a simple sample covar/correl matrx\n",
    "# also note, this is how most people make their covar matrix - the simple,naiive way!\n",
    "\n",
    "mean_baseline = data_valid.mean()\n",
    "cov_baseline = data_valid.cov()\n",
    "corr_baseline = data_valid.corr()\n",
    "var_marginals_baseline = np.diagonal(cov_baseline) # in this case same as pca\n",
    "\n",
    "log_like_baseline = model_log_like(cov_baseline,data_valid.mean(),len(data_valid.columns),data_test)    \n",
    "log_like_baseline.groupby(pd.Grouper(freq='B')).mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo: even though these tests don't make sense, leaving this here\n",
    "# until I figure out the best way to separate and compare across models\n",
    "# the cross sectional/corr error from marginal var forecast error\n",
    "\n",
    "covar_test = data_test.cov()\n",
    "var_marginals_test = np.diagonal(covar_test)\n",
    "corr_test = data_test.corr()\n",
    "\n",
    "#this doesnt necessarily make sense, but low on time so slapping these in here\n",
    "print(\"model0\")\n",
    "print(\"mad corr: {}\".format(np.abs(corr_test.values - corr0.values).sum()))\n",
    "print(\"mad marginals: {}\".format(np.abs(var_marginals_test - var_marginals0).sum()))\n",
    "\n",
    "print(\"baseline sample covar\")\n",
    "print(\"mad corr: {}\".format(np.abs(corr_test.values - corr_baseline.values).sum()))\n",
    "print(\"mad marginals: {}\".format(np.abs(var_marginals_test - var_marginals_baseline).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspection, how are these correlations?\n",
    "# i find it somewhat curious that test sample corr is much lower across all 0001HK pairs\n",
    "# in the pca model this woud be indicative of using too many PC's to reconstruct\n",
    "\n",
    "corr_test\n",
    "corr_baseline\n",
    "df_corr0 = pd.DataFrame(corr0).set_index(corr_test.index)\n",
    "df_corr0.columns = df_corr0.index\n",
    "\n",
    "pd.DataFrame({'test sample corr':corr_test['0001.HK'],'baseline':corr_baseline['0001.HK'],'pca0':df_corr0['0001.HK']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More todo: \n",
    "\n",
    "lets come back to the encoding reconstruction test. I need to think about this a little more.\n",
    "\n",
    "It is certainly useful for comparing the models, but as I am explaining as we go along I'm really most interested in which of these will give us the most useful correlation and var model for the future rather than which better describes the future (after it has happened).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test_reconst = model0.inverse_transform(data_test_encoded)\n",
    "#data_test_encoded\n",
    "#error_p0 = mean_squared_error(data_test_reconst, data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov0 = pd.DataFrame(data_test_encoded).cov()\n",
    "corr0= pd.DataFrame(data_test_encoded).corr()\n",
    "print(cov0)\n",
    "print(corr0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
