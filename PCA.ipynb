{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime as dt\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import *\n",
    "\n",
    "N_FACTORS=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set Normality Test\n",
      "Valid Set Normality Test\n",
      "0001.HK,(6.064224749785349, 0.048213685104578796)\n",
      "0857.HK,(8.452645800870712, 0.014605999639945466)\n",
      "Test  Set Normality Test\n",
      "0011.HK,(3.2554994473743633, 0.19637096624740646)\n",
      "0083.HK,(6.814479085838839, 0.03313253519634285)\n",
      "0386.HK,(1.5497081805806185, 0.4607710070582528)\n",
      "1928.HK,(4.337978009576364, 0.11429310830561656)\n",
      "3328.HK,(1.1835132218824163, 0.5533544020994401)\n",
      "3988.HK,(0.18692085297707353, 0.9107740592407177)\n",
      "Train Set Normality Test\n",
      "Valid Set Normality Test\n",
      "0001.HK,(6.064224749785349, 0.048213685104578796)\n",
      "0857.HK,(8.452645800870712, 0.014605999639945466)\n",
      "Test  Set Normality Test\n",
      "0011.HK,(3.2554994473743633, 0.19637096624740646)\n",
      "0083.HK,(6.814479085838839, 0.03313253519634285)\n",
      "0386.HK,(1.5497081805806185, 0.4607710070582528)\n",
      "1928.HK,(4.337978009576364, 0.11429310830561656)\n",
      "3328.HK,(1.1835132218824163, 0.5533544020994401)\n",
      "3988.HK,(0.18692085297707353, 0.9107740592407177)\n"
     ]
    }
   ],
   "source": [
    "%run TrainValidTestDataPrep.ipynb\n",
    "print(\"Train Set Normality Test\")\n",
    "for c in data_train.columns:\n",
    "    _stats = stats.jarque_bera(data_train)\n",
    "    if np.abs(_stats[1] > 0.01): print(\"%s,%s\"%(c, _stats))\n",
    "print(\"Valid Set Normality Test\")\n",
    "for c in data_valid.columns:\n",
    "    _stats = stats.jarque_bera(data_valid[c])\n",
    "    if np.abs(_stats[1] > 0.01): print(\"%s,%s\"%(c, _stats))\n",
    "print(\"Test  Set Normality Test\")\n",
    "for c in data_test.columns:\n",
    "    _stats = stats.jarque_bera(data_test[c])\n",
    "    if np.abs(_stats[1] > 0.01): print(\"%s,%s\"%(c, _stats))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SciKitLearn's PCA at prototype stage\n",
    "\n",
    "in future will just hand code with numpy as we will have more control that way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=5, random_state=None,\n",
       "  svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model0 = PCA(n_components=N_FACTORS)\n",
    "model0.fit(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_components: 5\n",
      "model covar: (50, 50)\n"
     ]
    }
   ],
   "source": [
    "dir(model0)\n",
    "print(\"n_components: \" + str(model0.n_components))\n",
    "print(\"model covar: \" + str(model0.get_covariance().shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change benchmark: encoding accuracy to encoding forecast accuracy\n",
    "\n",
    "Assume we can use any data up to the test_data cutoff(s). I'm using data_valid just because its the closest to data_test in time.\n",
    "\n",
    "Although I am interested in the sparce encoding of the return space to describe what has happened over the last hour, days, etc (and can use this as input into the forecast), at the end of the day I really care about the future. \n",
    "\n",
    "We are assuming in this PCA model case that we can model the whole market as 5 iid gaussian shocks + 50 individual shocks. I want to test that the covar matrix we produce is the best forecast of the forward covar matrix. And since we can split that into two separate forecasts: the correlations and the marginals, I'm really most interested in the accuracy and stability of the correlation matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded shape: (1141, 5)\n"
     ]
    }
   ],
   "source": [
    "data_train_encoded = pd.DataFrame(model0.transform(data_valid))\n",
    "print(\"encoded shape: \" + str(data_test_encoded.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.115503</td>\n",
       "      <td>0.190654</td>\n",
       "      <td>0.165122</td>\n",
       "      <td>-0.203549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.115503</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.101404</td>\n",
       "      <td>0.122249</td>\n",
       "      <td>0.118201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.190654</td>\n",
       "      <td>-0.101404</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.111239</td>\n",
       "      <td>-0.069077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.165122</td>\n",
       "      <td>0.122249</td>\n",
       "      <td>-0.111239</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.014165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.203549</td>\n",
       "      <td>0.118201</td>\n",
       "      <td>-0.069077</td>\n",
       "      <td>0.014165</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4\n",
       "0  1.000000  0.115503  0.190654  0.165122 -0.203549\n",
       "1  0.115503  1.000000 -0.101404  0.122249  0.118201\n",
       "2  0.190654 -0.101404  1.000000 -0.111239 -0.069077\n",
       "3  0.165122  0.122249 -0.111239  1.000000  0.014165\n",
       "4 -0.203549  0.118201 -0.069077  0.014165  1.000000"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#PCA factor covariance matrix \n",
    "cov_factor0 = data_train_encoded.cov()\n",
    "corr_factor0 = data_train_encoded.corr()\n",
    "corr_factor0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# note: i did it this way (factor_exp * covar_common * factor_exp) for illustration\n",
    "factor_exposures0 = model0.components_\n",
    "cov_from_common_factors0 = pd.DataFrame(np.dot(np.dot(factor_exposures0.T,cov_factor0),factor_exposures0))\n",
    "\n",
    "# note: can also do like below, as you can see, they are equivalent\n",
    "np.isclose(pd.DataFrame(model0.inverse_transform(data_train_encoded)).cov(),cov_from_common_factors).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#because correlation comes from cross sectional effects only we can create like this (one of many ways):\n",
    "var_marginals0 = np.diagonal(data_valid.cov())\n",
    "stdev_marginals0 = np.sqrt(var_marginals0)\n",
    "corr0 = cov_from_common_factors0 / np.outer(stdev_marginals0,stdev_marginals0)\n",
    "np.fill_diagonal(corr0.values,1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>marginal</th>\n",
       "      <th>specific</th>\n",
       "      <th>pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018.HK</th>\n",
       "      <td>0.00007</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>-0.076968</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         marginal  specific       pct\n",
       "2018.HK   0.00007 -0.000005 -0.076968"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We can also shave off the specific var for use later as well\n",
    "var_specific0 = np.diagonal(data_valid.cov()) - np.diagonal(cov_from_common_factors0)\n",
    "\n",
    "#inspect\n",
    "var_breakdown = pd.DataFrame({'marginal':var_marginals0,\n",
    "                              'specific':var_specific0,\n",
    "                              'pct':var_specific0/var_marginals0}).set_index(data_valid.columns)\n",
    "    \n",
    "var_breakdown[var_breakdown['pct']< 0.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "\n",
    "todo: this needs a lot of work. I'm low on time so just mindlessly banging this out for now.\n",
    "\n",
    "the end goal is to be able to forecast the covariance matrix (for optimization or simulation etc), so I am sure there are better metrics than the ones i have below. Like likelhood of this covariance structure producing the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse corr: 394.6182294790748\n",
      "mse marginals: 0.000454458558671855\n"
     ]
    }
   ],
   "source": [
    "covar_test = data_test.cov()\n",
    "var_marginals_test = np.diagonal(covar_test)\n",
    "corr_test = data_test.corr()\n",
    "\n",
    "#this doesnt necessarily make sense, but low on time so slapping these in here\n",
    "print(\"mse corr: {}\".format(np.abs(corr_test.values - corr0.values).sum()))\n",
    "print(\"mse marginals: {}\".format(np.abs(var_marginals_test - var_marginals0).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse corr: 318.079674650662\n",
      "mse marginals: 0.000454458558671855\n"
     ]
    }
   ],
   "source": [
    "# this shoudl really be a separate model (and I'll make it one when I have a minute)\n",
    "# but lets comp the above to a simple sample covar/correl matrx\n",
    "\n",
    "corr_baseline = data_valid.corr()\n",
    "var_marginals_baseline = np.diagonal(data_valid.cov()) # in this case same as pca\n",
    "\n",
    "# this doesnt make sense - see qualifier above\n",
    "print(\"mse corr: {}\".format(np.abs(corr_test.values - corr_baseline.values).sum()))\n",
    "print(\"mse marginals: {}\".format(np.abs(var_marginals_test - var_marginals_baseline).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test sample corr</th>\n",
       "      <th>baseline</th>\n",
       "      <th>pca0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0001.HK</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0002.HK</th>\n",
       "      <td>0.217412</td>\n",
       "      <td>0.160217</td>\n",
       "      <td>0.262418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0003.HK</th>\n",
       "      <td>0.146921</td>\n",
       "      <td>0.299431</td>\n",
       "      <td>0.351168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0005.HK</th>\n",
       "      <td>0.143474</td>\n",
       "      <td>0.469610</td>\n",
       "      <td>0.403459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0006.HK</th>\n",
       "      <td>0.261169</td>\n",
       "      <td>0.352192</td>\n",
       "      <td>0.337022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0011.HK</th>\n",
       "      <td>0.227127</td>\n",
       "      <td>0.472645</td>\n",
       "      <td>0.426893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0012.HK</th>\n",
       "      <td>0.277419</td>\n",
       "      <td>0.495431</td>\n",
       "      <td>0.401665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0016.HK</th>\n",
       "      <td>0.286999</td>\n",
       "      <td>0.464531</td>\n",
       "      <td>0.453960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0017.HK</th>\n",
       "      <td>0.300431</td>\n",
       "      <td>0.507741</td>\n",
       "      <td>0.508973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0019.HK</th>\n",
       "      <td>0.165106</td>\n",
       "      <td>0.441108</td>\n",
       "      <td>0.397571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0023.HK</th>\n",
       "      <td>0.256692</td>\n",
       "      <td>0.486244</td>\n",
       "      <td>0.487984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0027.HK</th>\n",
       "      <td>0.132876</td>\n",
       "      <td>0.397265</td>\n",
       "      <td>0.306487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0066.HK</th>\n",
       "      <td>0.082839</td>\n",
       "      <td>0.350236</td>\n",
       "      <td>0.386785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0083.HK</th>\n",
       "      <td>0.163112</td>\n",
       "      <td>0.534757</td>\n",
       "      <td>0.579066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0101.HK</th>\n",
       "      <td>0.127232</td>\n",
       "      <td>0.405899</td>\n",
       "      <td>0.362631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0144.HK</th>\n",
       "      <td>0.124107</td>\n",
       "      <td>0.436977</td>\n",
       "      <td>0.409808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0151.HK</th>\n",
       "      <td>0.123524</td>\n",
       "      <td>0.418586</td>\n",
       "      <td>0.376334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0175.HK</th>\n",
       "      <td>0.158629</td>\n",
       "      <td>0.418218</td>\n",
       "      <td>0.413749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0267.HK</th>\n",
       "      <td>0.194641</td>\n",
       "      <td>0.491145</td>\n",
       "      <td>0.466816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0288.HK</th>\n",
       "      <td>0.099000</td>\n",
       "      <td>0.397349</td>\n",
       "      <td>0.268131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0386.HK</th>\n",
       "      <td>0.092671</td>\n",
       "      <td>0.477523</td>\n",
       "      <td>0.422900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0388.HK</th>\n",
       "      <td>0.147101</td>\n",
       "      <td>0.428865</td>\n",
       "      <td>0.450314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0688.HK</th>\n",
       "      <td>0.083514</td>\n",
       "      <td>0.419950</td>\n",
       "      <td>0.462765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0700.HK</th>\n",
       "      <td>0.121445</td>\n",
       "      <td>0.419333</td>\n",
       "      <td>0.444006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0762.HK</th>\n",
       "      <td>0.101447</td>\n",
       "      <td>0.433094</td>\n",
       "      <td>0.461388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0823.HK</th>\n",
       "      <td>0.133148</td>\n",
       "      <td>0.377090</td>\n",
       "      <td>0.366386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0836.HK</th>\n",
       "      <td>0.116031</td>\n",
       "      <td>0.355198</td>\n",
       "      <td>0.216672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0857.HK</th>\n",
       "      <td>0.207941</td>\n",
       "      <td>0.492677</td>\n",
       "      <td>0.477297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0883.HK</th>\n",
       "      <td>0.048320</td>\n",
       "      <td>0.488541</td>\n",
       "      <td>0.477979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0939.HK</th>\n",
       "      <td>0.246713</td>\n",
       "      <td>0.327419</td>\n",
       "      <td>0.381215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0941.HK</th>\n",
       "      <td>0.177215</td>\n",
       "      <td>0.407463</td>\n",
       "      <td>0.469720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1038.HK</th>\n",
       "      <td>0.213479</td>\n",
       "      <td>0.294043</td>\n",
       "      <td>0.309539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1044.HK</th>\n",
       "      <td>0.144194</td>\n",
       "      <td>0.449557</td>\n",
       "      <td>0.390151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1088.HK</th>\n",
       "      <td>0.218833</td>\n",
       "      <td>0.391657</td>\n",
       "      <td>0.470075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1093.HK</th>\n",
       "      <td>-0.030822</td>\n",
       "      <td>0.315495</td>\n",
       "      <td>0.354883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1109.HK</th>\n",
       "      <td>0.126013</td>\n",
       "      <td>0.450879</td>\n",
       "      <td>0.533464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1113.HK</th>\n",
       "      <td>0.376455</td>\n",
       "      <td>0.478770</td>\n",
       "      <td>0.444235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1299.HK</th>\n",
       "      <td>0.187013</td>\n",
       "      <td>0.429522</td>\n",
       "      <td>0.439461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1398.HK</th>\n",
       "      <td>0.225751</td>\n",
       "      <td>0.315640</td>\n",
       "      <td>0.387655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1928.HK</th>\n",
       "      <td>0.117652</td>\n",
       "      <td>0.373716</td>\n",
       "      <td>0.321279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997.HK</th>\n",
       "      <td>0.205690</td>\n",
       "      <td>0.349802</td>\n",
       "      <td>0.496759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007.HK</th>\n",
       "      <td>0.158822</td>\n",
       "      <td>0.342482</td>\n",
       "      <td>0.355274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018.HK</th>\n",
       "      <td>0.104577</td>\n",
       "      <td>0.373966</td>\n",
       "      <td>0.370544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2318.HK</th>\n",
       "      <td>0.182207</td>\n",
       "      <td>0.380392</td>\n",
       "      <td>0.452407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2319.HK</th>\n",
       "      <td>0.117959</td>\n",
       "      <td>0.289404</td>\n",
       "      <td>0.383787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2382.HK</th>\n",
       "      <td>0.137904</td>\n",
       "      <td>0.331854</td>\n",
       "      <td>0.376279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2388.HK</th>\n",
       "      <td>0.283320</td>\n",
       "      <td>0.355040</td>\n",
       "      <td>0.448285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2628.HK</th>\n",
       "      <td>0.172742</td>\n",
       "      <td>0.440948</td>\n",
       "      <td>0.466359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3328.HK</th>\n",
       "      <td>0.191054</td>\n",
       "      <td>0.407401</td>\n",
       "      <td>0.455998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3988.HK</th>\n",
       "      <td>0.210482</td>\n",
       "      <td>0.363664</td>\n",
       "      <td>0.375743</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         test sample corr  baseline      pca0\n",
       "0001.HK          1.000000  1.000000  1.000000\n",
       "0002.HK          0.217412  0.160217  0.262418\n",
       "0003.HK          0.146921  0.299431  0.351168\n",
       "0005.HK          0.143474  0.469610  0.403459\n",
       "0006.HK          0.261169  0.352192  0.337022\n",
       "0011.HK          0.227127  0.472645  0.426893\n",
       "0012.HK          0.277419  0.495431  0.401665\n",
       "0016.HK          0.286999  0.464531  0.453960\n",
       "0017.HK          0.300431  0.507741  0.508973\n",
       "0019.HK          0.165106  0.441108  0.397571\n",
       "0023.HK          0.256692  0.486244  0.487984\n",
       "0027.HK          0.132876  0.397265  0.306487\n",
       "0066.HK          0.082839  0.350236  0.386785\n",
       "0083.HK          0.163112  0.534757  0.579066\n",
       "0101.HK          0.127232  0.405899  0.362631\n",
       "0144.HK          0.124107  0.436977  0.409808\n",
       "0151.HK          0.123524  0.418586  0.376334\n",
       "0175.HK          0.158629  0.418218  0.413749\n",
       "0267.HK          0.194641  0.491145  0.466816\n",
       "0288.HK          0.099000  0.397349  0.268131\n",
       "0386.HK          0.092671  0.477523  0.422900\n",
       "0388.HK          0.147101  0.428865  0.450314\n",
       "0688.HK          0.083514  0.419950  0.462765\n",
       "0700.HK          0.121445  0.419333  0.444006\n",
       "0762.HK          0.101447  0.433094  0.461388\n",
       "0823.HK          0.133148  0.377090  0.366386\n",
       "0836.HK          0.116031  0.355198  0.216672\n",
       "0857.HK          0.207941  0.492677  0.477297\n",
       "0883.HK          0.048320  0.488541  0.477979\n",
       "0939.HK          0.246713  0.327419  0.381215\n",
       "0941.HK          0.177215  0.407463  0.469720\n",
       "1038.HK          0.213479  0.294043  0.309539\n",
       "1044.HK          0.144194  0.449557  0.390151\n",
       "1088.HK          0.218833  0.391657  0.470075\n",
       "1093.HK         -0.030822  0.315495  0.354883\n",
       "1109.HK          0.126013  0.450879  0.533464\n",
       "1113.HK          0.376455  0.478770  0.444235\n",
       "1299.HK          0.187013  0.429522  0.439461\n",
       "1398.HK          0.225751  0.315640  0.387655\n",
       "1928.HK          0.117652  0.373716  0.321279\n",
       "1997.HK          0.205690  0.349802  0.496759\n",
       "2007.HK          0.158822  0.342482  0.355274\n",
       "2018.HK          0.104577  0.373966  0.370544\n",
       "2318.HK          0.182207  0.380392  0.452407\n",
       "2319.HK          0.117959  0.289404  0.383787\n",
       "2382.HK          0.137904  0.331854  0.376279\n",
       "2388.HK          0.283320  0.355040  0.448285\n",
       "2628.HK          0.172742  0.440948  0.466359\n",
       "3328.HK          0.191054  0.407401  0.455998\n",
       "3988.HK          0.210482  0.363664  0.375743"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspection, how are these correlations\n",
    "\n",
    "corr_test\n",
    "corr_baseline\n",
    "df_corr0 = pd.DataFrame(corr0).set_index(corr_test.index)\n",
    "df_corr0.columns = df_corr0.index\n",
    "\n",
    "pd.DataFrame({'test sample corr':corr_test['0001.HK'],'baseline':corr_baseline['0001.HK'],'pca0':df_corr0['0001.HK']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More todo: \n",
    "\n",
    "lets come back to the encoding reconstruction test. I need to think about this a little more.\n",
    "\n",
    "It is certainly useful for comparing the models, but as I am explaining as we go along I'm really most interested in which of these will give us the most useful correlation and var model for the future rather than which better describes the future (after it has happened).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00667647,  0.01836518,  0.00308754, -0.00454558, -0.0062883 ],\n",
       "       [ 0.0113555 ,  0.01830825, -0.00388114, -0.00300052, -0.00108511],\n",
       "       [ 0.01897593,  0.01470508, -0.00817796, -0.00025326, -0.00533718],\n",
       "       ...,\n",
       "       [ 0.00673279,  0.00253529,  0.00045948, -0.00736422,  0.00071269],\n",
       "       [ 0.02088822, -0.00491727,  0.00835285, -0.00333194, -0.00184507],\n",
       "       [ 0.0248835 , -0.00156089,  0.00964574, -0.00334135, -0.01034274]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test_reconst = model0.inverse_transform(data_test_encoded)\n",
    "#data_test_encoded\n",
    "#error_p0 = mean_squared_error(data_test_reconst, data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0         1         2         3         4\n",
      "0  0.000679 -0.000072  0.000075  0.000007 -0.000079\n",
      "1 -0.000072  0.000085 -0.000030 -0.000012  0.000015\n",
      "2  0.000075 -0.000030  0.000085  0.000002 -0.000012\n",
      "3  0.000007 -0.000012  0.000002  0.000049 -0.000003\n",
      "4 -0.000079  0.000015 -0.000012 -0.000003  0.000042\n",
      "          0         1         2         3         4\n",
      "0  1.000000 -0.300542  0.312821  0.037054 -0.465941\n",
      "1 -0.300542  1.000000 -0.351472 -0.189661  0.247041\n",
      "2  0.312821 -0.351472  1.000000  0.023385 -0.198348\n",
      "3  0.037054 -0.189661  0.023385  1.000000 -0.057644\n",
      "4 -0.465941  0.247041 -0.198348 -0.057644  1.000000\n"
     ]
    }
   ],
   "source": [
    "cov0 = pd.DataFrame(data_test_encoded).cov()\n",
    "corr0= pd.DataFrame(data_test_encoded).corr()\n",
    "print(cov0)\n",
    "print(corr0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
